{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction of Snorkel\n",
    "snorkel은 manual labeling이 없이 데이터셋을 프로그래밍틱하게 라벨링하는 방법론임.\n",
    "\n",
    "스노클은 현재 아래와 같은 세가지의 operation을 제공함.\n",
    "<img src=\"https://www.snorkel.org/doks-theme/assets/images/layout/Overview.png\">\n",
    "\n",
    "1. Labeling Data\n",
    "    - rule 또는 거리기반의 supervision technique을 활용하여 처리함.\n",
    "2. Transforming Data\n",
    "    - 데이터 augmenatation\n",
    "    - NLP에서 효과적인 데이터 augmentation은 어떤 방법일까?\n",
    "3. Slicing Data\n",
    "    - 전체 데이터에서 determinant plane에 대한 uncertainty가 높은 부분을 따로 subset하는 방법."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유튜브 데이터를 활용한 스팸 분류 모델\n",
    "\n",
    "아래와 같은 기본적인 다섯가지 스텝을 따른다.\n",
    "\n",
    "1. Writing Labeling Functions (LFs)\n",
    "- 수작업으로 training data를 라벨링 하는 것이 아닌 프로그래밍적인 방법으로 라벨링을 진행함.\n",
    "\n",
    "2. Modeling & Combining LFs\n",
    "- Next, we’ll use Snorkel’s LabelModel to automatically learn the accuracies of our LFs and reweight and combine their outputs into a single, confidence-weighted training label per data point.\n",
    "\n",
    "3. Writing Transformation Functions (TFs) for Data Augmentation\n",
    "- Then, we’ll augment this labeled training set by writing a simple TF.\n",
    "\n",
    "4. Writing Slicing Functions (SFs) for Data Subset Selection\n",
    "- We’ll also preview writing an SF to identify a critical subset or slice of our training set.\n",
    "\n",
    "5. Training a final ML model\n",
    "- Finally, we’ll train an ML model with our training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Don't truncate text fields in the display\n",
    "pd.set_option(\"display.max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = sorted(glob.glob(\"data/Youtube*.csv\"))\n",
    "dfs = []\n",
    "for i, filename in enumerate(filenames, start=1):\n",
    "    df = pd.read_csv(filename)\n",
    "    # Lowercase column names\n",
    "    df.columns = map(str.lower, df.columns)\n",
    "    # Rename fields\n",
    "    df = df.rename(columns={\"class\": \"label\", \"content\": \"text\"})\n",
    "    # Remove comment_id, label fields\n",
    "    df = df.drop(\"comment_id\", axis=1)\n",
    "    df = df.drop(\"label\", axis=1)\n",
    "    # Shuffle order\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "    dfs.append(df)\n",
    "\n",
    "df_train = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alessandro leite</td>\n",
       "      <td>2014-11-05T22:21:36</td>\n",
       "      <td>pls http://www10.vakinha.com.br/VaquinhaE.aspx?e=313327 help me get vip gun  cross fire al﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Salim Tayara</td>\n",
       "      <td>2014-11-02T14:33:30</td>\n",
       "      <td>if your like drones, plz subscribe to Kamal Tayara. He takes videos with  his drone that are absolutely beautiful.﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Phuc Ly</td>\n",
       "      <td>2014-01-20T15:27:47</td>\n",
       "      <td>go here to check the views :3﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DropShotSk8r</td>\n",
       "      <td>2014-01-19T04:27:18</td>\n",
       "      <td>Came here to check the views, goodbye.﻿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>css403</td>\n",
       "      <td>2014-11-07T14:25:48</td>\n",
       "      <td>i am 2,126,492,636 viewer :D﻿</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author                 date  \\\n",
       "0  Alessandro leite  2014-11-05T22:21:36   \n",
       "1  Salim Tayara      2014-11-02T14:33:30   \n",
       "2  Phuc Ly           2014-01-20T15:27:47   \n",
       "3  DropShotSk8r      2014-01-19T04:27:18   \n",
       "4  css403            2014-11-07T14:25:48   \n",
       "\n",
       "                                                                                                                  text  \n",
       "0  pls http://www10.vakinha.com.br/VaquinhaE.aspx?e=313327 help me get vip gun  cross fire al﻿                          \n",
       "1  if your like drones, plz subscribe to Kamal Tayara. He takes videos with  his drone that are absolutely beautiful.﻿  \n",
       "2  go here to check the views :3﻿                                                                                       \n",
       "3  Came here to check the views, goodbye.﻿                                                                              \n",
       "4  i am 2,126,492,636 viewer :D﻿                                                                                        "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Writing Labeling Functions\n",
    "- Labeling function은 데이터에 대한 휴리스틱과 다양한 규칙을 부여할 수 있음.\n",
    "    - 즉, 도메인에 대한 사전지식을 바탕으로 labeling을 진행할 수 있음.\n",
    "- Labeling Function의 핵심 아이디어는 규칙이 완벽하게 정확하지 않아도 된다는 점이다. 심지어 규칙들 간의 높은 상관성도 문제가 없다.(why?)\n",
    "    - Snorkel이 자동적으로 데이터의 정확도와 상관성을 일관성 있는 방법으로 예측하고, labeling을 달아준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from snorkel.labeling import labeling_function\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the label mappings for convenience\n",
    "ABSTAIN = -1\n",
    "NOT_SPAM = 0\n",
    "SPAM = 1\n",
    "\n",
    "# 키워드 매칭\n",
    "@labeling_function()\n",
    "def lf_keyword_my(x):\n",
    "    \"\"\"Many spam comments talk about 'my channel', 'my video', etc.\"\"\"\n",
    "    return SPAM if \"my\" in x.text.lower() else ABSTAIN\n",
    "\n",
    "# 정규표현식\n",
    "@labeling_function()\n",
    "def lf_regex_check_out(x):\n",
    "    \"\"\"Spam comments say 'check out my video', 'check it out', etc.\"\"\"\n",
    "    return SPAM if re.search(r\"check.*out\", x.text, flags=re.I) else ABSTAIN\n",
    "\n",
    "# 임의의 휴리스틱\n",
    "@labeling_function()\n",
    "def lf_short_comment(x):\n",
    "    \"\"\"Non-spam comments are often short, such as 'cool video!'.\"\"\"\n",
    "    return NOT_SPAM if len(x.text.split()) < 5 else ABSTAIN\n",
    "\n",
    "# Third-party 모델\n",
    "@labeling_function()\n",
    "def lf_textblob_polarity(x):\n",
    "    \"\"\"\n",
    "    We use a third-party sentiment classification model, TextBlob.\n",
    "\n",
    "    We combine this with the heuristic that non-spam comments are often positive.\n",
    "    \"\"\"\n",
    "    return NOT_SPAM if TextBlob(x.text).sentiment.polarity > 0.3 else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Combining & Cleaning the Labels\n",
    "- Unlabeled training data에 우리가 만든 labeling function들을 적용한다.\n",
    "    - 그러면, label matrix가 나옴.\n",
    "    - 예를 들면, 이 케이스의 경우 label_function이 4개니까, $(m, 4)$의 label_matrix가 생성됨.\n",
    "- LabelModel을 활용해서 label들을 reweight and combine해서 하나 label로 예측하고, 통합함.\n",
    "    - $(m, 4) \\longrightarrow (m, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1956/1956 [00:01<00:00, 1058.84it/s]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling.model import LabelModel\n",
    "from snorkel.labeling import PandasLFApplier\n",
    "\n",
    "# Define the set of labeling functions (LFs)\n",
    "lfs = [lf_keyword_my, lf_regex_check_out, lf_short_comment, lf_textblob_polarity]\n",
    "\n",
    "# Apply the LFs to the unlabeled training data\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_train = applier.apply(df_train)\n",
    "\n",
    "# Train the label model and compute the training labels\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train, n_epochs=500, log_freq=50, seed=123)\n",
    "df_train[\"label\"] = label_model.predict(L=L_train, tie_break_policy=\"abstain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ABSTAIN은 하나로 labeling하기 애매한 경우에 사용함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train.label != ABSTAIN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Writing Transformation Functions for Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 개인적으로 동의어 replace 기반의 Data Augmentation 방법론은 잘 모르겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Writing a Slicing Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 머신러닝 문제에서 데이터셋의 특정 부분이 다른 부분보다 더 중요한 경우가 많음.\n",
    "- 예를 들면, 유튜브 스팸 필터링 데이터에서 shortened url 패턴이 존재하면 없는 경우보다 스팸일 가능성(likelihood)가 높음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.slicing import slicing_function\n",
    "\n",
    "\n",
    "@slicing_function()\n",
    "def short_link(x):\n",
    "    \"\"\"Return whether text matches common pattern for shortened \".ly\" links.\"\"\"\n",
    "    return int(bool(re.search(r\"\\w+\\.ly\", x.text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 구체적인 사용방법은 SF tutorial에서 진행함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Training a Classifier\n",
    "- 너가 원하는 모델의 input 데이터로 사용해서 모델링 해봐"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
